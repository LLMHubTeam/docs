---
title: 'Completion'
openapi: 'POST /completions'
description: 'Generate text completions using various AI models through OpenRouter'
---

## Overview

The Completion API allows you to generate text completions from a given prompt using various AI models. This endpoint is compatible with the OpenAI completions format and supports multiple model providers.

## Authentication

All requests require authentication using a Bearer token in the Authorization header:

```bash
Authorization: Bearer YOUR_API_KEY
```

## Request Parameters

<ParamField body="model" type="string" required>
    The model to use for text generation. Examples:
    - `openai/gpt-3.5-turbo-instruct`
    - `openai/gpt-4`
    - `anthropic/claude-3-sonnet`
    - `meta-llama/llama-2-70b-chat`
</ParamField>

<ParamField body="prompt" type="string" required>
    The input text prompt to generate completions for. Can be a string or array of strings.
</ParamField>

<ParamField body="max_tokens" type="integer" default="16">
    The maximum number of tokens to generate in the completion. The token count of your prompt plus max_tokens cannot exceed the model's context length.
</ParamField>

<ParamField body="temperature" type="number" default="1">
    What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
</ParamField>

<ParamField body="top_p" type="number" default="1">
    An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
</ParamField>

<ParamField body="n" type="integer" default="1">
    How many completions to generate for each prompt. Maximum value is 128.
</ParamField>

<ParamField body="stream" type="boolean" default="false">
    Whether to stream back partial progress. If set, tokens will be sent as data-only server-sent events as they become available.
</ParamField>

<ParamField body="logprobs" type="integer">
    Include the log probabilities on the logprobs most likely tokens, as well the chosen tokens. For example, if logprobs is 5, the API will return a list of the 5 most likely tokens.
</ParamField>

<ParamField body="echo" type="boolean" default="false">
    Echo back the prompt in addition to the completion.
</ParamField>

<ParamField body="stop" type="string or array">
    Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.
</ParamField>

<ParamField body="presence_penalty" type="number" default="0">
    Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
</ParamField>

<ParamField body="frequency_penalty" type="number" default="0">
    Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
</ParamField>

<ParamField body="best_of" type="integer" default="1">
    Generates best_of completions server-side and returns the "best" (the one with the highest log probability per token). Results cannot be streamed.
</ParamField>

<ParamField body="logit_bias" type="map">
    Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens to an associated bias value from -100 to 100.
</ParamField>

<ParamField body="user" type="string">
    A unique identifier representing your end-user, which can help OpenRouter to monitor and detect abuse.
</ParamField>

## Response Format

<ResponseField name="id" type="string">
    A unique identifier for the completion.
</ResponseField>

<ResponseField name="object" type="string">
    The object type, which is always "text_completion".
</ResponseField>

<ResponseField name="created" type="integer">
    The Unix timestamp (in seconds) of when the completion was created.
</ResponseField>

<ResponseField name="model" type="string">
    The model used for the completion.
</ResponseField>

<ResponseField name="choices" type="array">
    The list of completion choices, sorted by their log probabilities in descending order.

    <Expandable title="Choice Object">
        <ResponseField name="text" type="string">
            The generated text completion.
        </ResponseField>

        <ResponseField name="index" type="integer">
            The index of this choice in the choices array.
        </ResponseField>

        <ResponseField name="logprobs" type="object" nullable>
            The log probabilities for the tokens in the completion, if requested.

            <Expandable title="Logprobs Object">
                <ResponseField name="tokens" type="array">
                    The tokens chosen by the completion.
                </ResponseField>

                <ResponseField name="token_logprobs" type="array">
                    The log probability of each token.
                </ResponseField>

                <ResponseField name="top_logprobs" type="array">
                    The top logprobs for each token.
                </ResponseField>

                <ResponseField name="text_offset" type="array">
                    The character offsets of each token.
                </ResponseField>
            </Expandable>
        </ResponseField>

        <ResponseField name="finish_reason" type="string">
            The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, or content_filter if content was omitted due to a flag from our content filters.
        </ResponseField>
    </Expandable>
</ResponseField>

<ResponseField name="usage" type="object">
    Usage statistics for the completion request.

    <Expandable title="Usage Object">
        <ResponseField name="prompt_tokens" type="integer">
            The number of tokens in the prompt.
        </ResponseField>

        <ResponseField name="completion_tokens" type="integer">
            The number of tokens in the generated completion.
        </ResponseField>

        <ResponseField name="total_tokens" type="integer">
            The total number of tokens used in the request (prompt + completion).
        </ResponseField>
    </Expandable>
</ResponseField>

## Code Examples

<CodeGroup>

    ```bash cURL
    curl https://openrouter.ai/api/v1/completions \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OPENROUTER_API_KEY" \
    -H "HTTP-Referer: $YOUR_SITE_URL" \
    -H "X-Title: $YOUR_SITE_NAME" \
    -d '{
    "model": "openai/gpt-3.5-turbo-instruct",
    "prompt": "Say this is a test",
    "max_tokens": 7,
    "temperature": 0
}'
    ```

    ```python Python
    import openai

    client = openai.OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key="YOUR_OPENROUTER_API_KEY",
    )

    completion = client.completions.create(
    model="openai/gpt-3.5-turbo-instruct",
    prompt="Say this is a test",
    max_tokens=7,
    temperature=0
    )

    print(completion.choices[0].text)
    ```

    ```javascript JavaScript
    const response = await fetch("https://openrouter.ai/api/v1/completions", {
    method: "POST",
    headers: {
    "Authorization": `Bearer ${OPENROUTER_API_KEY}`,
    "HTTP-Referer": `${YOUR_SITE_URL}`,
    "X-Title": `${YOUR_SITE_NAME}`,
    "Content-Type": "application/json"
},
    body: JSON.stringify({
    "model": "openai/gpt-3.5-turbo-instruct",
    "prompt": "Say this is a test",
    "max_tokens": 7,
    "temperature": 0
})
});

    const data = await response.json();
    console.log(data.choices[0].text);
    ```

    ```go Go
    package main

    import (
    "bytes"
    "encoding/json"
    "fmt"
    "io"
    "net/http"
    )

    func main() {
    url := "https://openrouter.ai/api/v1/completions"

    requestBody := map[string]interface{}{
    "model":       "openai/gpt-3.5-turbo-instruct",
    "prompt":      "Say this is a test",
    "max_tokens":  7,
    "temperature": 0,
}

    jsonBody, _ := json.Marshal(requestBody)

    req, _ := http.NewRequest("POST", url, bytes.NewBuffer(jsonBody))
    req.Header.Set("Content-Type", "application/json")
    req.Header.Set("Authorization", "Bearer "+YOUR_OPENROUTER_API_KEY)
    req.Header.Set("HTTP-Referer", YOUR_SITE_URL)
    req.Header.Set("X-Title", YOUR_SITE_NAME)

    client := &http.Client{}
    resp, _ := client.Do(req)
    defer resp.Body.Close()

    body, _ := io.ReadAll(resp.Body)
    fmt.Println(string(body))
}
    ```

</CodeGroup>

## Response Example

```json
{
  "id": "cmpl-uqkvlQyYK7bGYrRHQ0eXlWi7",
  "object": "text_completion",
  "created": 1589478378,
  "model": "openai/gpt-3.5-turbo-instruct",
  "choices": [
    {
      "text": "\n\nThis is indeed a test",
      "index": 0,
      "logprobs": null,
      "finish_reason": "length"
    }
  ],
  "usage": {
    "prompt_tokens": 5,
    "completion_tokens": 7,
    "total_tokens": 12
  }
}
```

## Streaming Example

When `stream` is set to `true`, the API will return Server-Sent Events:

```bash
curl https://openrouter.ai/api/v1/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $OPENROUTER_API_KEY" \
  -H "HTTP-Referer: $YOUR_SITE_URL" \
  -H "X-Title: $YOUR_SITE_NAME" \
  -d '{
    "model": "openai/gpt-3.5-turbo-instruct",
    "prompt": "Say this is a test",
    "max_tokens": 7,
    "temperature": 0,
    "stream": true
  }'
```

The response will be a series of Server-Sent Events:

```
data: {"id":"cmpl-7iA7iJjj8V2zOkz0gzaD8xglSu","object":"text_completion","created":1690759702,"choices":[{"text":"This","index":0,"logprobs":null,"finish_reason":null}],"model":"openai/gpt-3.5-turbo-instruct"}

data: {"id":"cmpl-7iA7iJjj8V2zOkz0gzaD8xglSu","object":"text_completion","created":1690759702,"choices":[{"text":" is","index":0,"logprobs":null,"finish_reason":null}],"model":"openai/gpt-3.5-turbo-instruct"}

data: {"id":"cmpl-7iA7iJjj8V2zOkz0gzaD8xglSu","object":"text_completion","created":1690759702,"choices":[{"text":" a","index":0,"logprobs":null,"finish_reason":null}],"model":"openai/gpt-3.5-turbo-instruct"}

data: {"id":"cmpl-7iA7iJjj8V2zOkz0gzaD8xglSu","object":"text_completion","created":1690759702,"choices":[{"text":" test","index":0,"logprobs":null,"finish_reason":"stop"}],"model":"openai/gpt-3.5-turbo-instruct"}

data: [DONE]
```

## Error Handling

The API returns standard HTTP status codes:

- `200` - Success
- `400` - Bad Request: Invalid parameters
- `401` - Unauthorized: Invalid API key
- `402` - Payment Required: Insufficient credits
- `429` - Too Many Requests: Rate limit exceeded
- `500` - Internal Server Error

Example error response:

```json
{
  "error": {
    "message": "Invalid model specified: invalid-model",
    "type": "invalid_request_error",
    "param": "model",
    "code": "model_not_found"
  }
}
```

## Notes

- The Completion endpoint follows the OpenAI API format for compatibility
- For chat-based interactions, consider using the [Chat Completions API](/api-reference/chat) instead
- Different models have different capabilities, context lengths, and pricing
- Some models may not support all parameters (e.g., logprobs, best_of)
- Streaming responses use Server-Sent Events format
- Always include proper headers for monitoring and abuse detection